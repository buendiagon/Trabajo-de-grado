\section*{Introducción}
\addcontentsline{toc}{section}{Introducción}

% 1. Origen de la computación autonómica (Citation needed for industry tendencies)

La simulación cuántica en computadoras clásicas enfrenta un desafío fundamental debido al crecimiento exponencial del espacio de estados conforme aumenta el número de qubits. Por ejemplo, la representación de un sistema cuántico de 40 qubits requiere aproximadamente $8 \, \text{bytes (double)} \times 2 \, (\text{real}, \, \text{imag}) \times 2^{40} = 16 \, \text{TB}$ de memoria RAM, mientras que para 50 qubits la demanda se dispara a petabytes (PB), superando la capacidad de las computadoras convencionales \cite{wu2019full}. Esta limitación de memoria restringe el estudio de algoritmos cuánticos avanzados y su validación en hardware clásico antes de su implementación en procesadores cuánticos \cite{zhang2024bmqsim}. Ante esta problemática, es necesario desarrollar estrategias eficientes de gestión de memoria que permitan extender el alcance de las simulaciones cuánticas sin comprometer la viabilidad computacional.

Considerando la creciente importancia de la computación cuántica, optimizar la memoria en simuladores cuánticos no solo permite ampliar el rango de algoritmos simulables en hardware clásico, sino que también reduce la dependencia exclusiva de procesadores cuánticos experimentales. Además, una gestión eficiente de memoria podría reducir significativamente los costos computacionales y el tiempo de simulación en supercomputadoras, lo que resulta en un beneficio directo tanto para la investigación en computación cuántica como para aplicaciones prácticas en la industria y la ciencia.

Para abordar este problema, se han explorado diversas estrategias que pueden agruparse en varias familias de métodos. Entre ellas, la compresión de datos ha sido ampliamente estudiada con enfoques como la compresión con pérdida controlada (e.g., SZ, ZFP), que permite reducir la carga de almacenamiento de vectores de estado sin afectar significativamente la fidelidad de los cálculos \cite{wu2018amplitude}. Otra línea de investigación relevante es la de los métodos de podado de estados cuánticos, los cuales eliminan amplitudes irrelevantes para disminuir el consumo de memoria, aunque con un impacto variable en la precisión del resultado \cite{song2023mera}. Asimismo, la computación de alto rendimiento (HPC o High Performance Computing) ha demostrado ser una solución clave, empleando distribución de datos entre nodos y optimizaciones en el acceso a memoria para maximizar la eficiencia de la simulación en infraestructuras de supercomputadoras \cite{diaz2024software}.

A pesar de los avances en estas áreas, muchas de estas estrategias introducen errores acumulativos en la simulación, lo que puede comprometer la validez de los resultados. En este contexto, esta investigación propone una estrategia basada en compresión de datos sin pérdida de precisión, con el objetivo de optimizar el almacenamiento de vectores de estado sin alterar las amplitudes cuánticas originales. Para ello, el proyecto se desarrollará en varias etapas. Inicialmente, se llevará a cabo una revisión del estado del arte para identificar las técnicas de compresión más relevantes. Luego, se seleccionará un simulador cuántico adecuado que permita modificaciones en su gestión de memoria para evaluar dichas técnicas. Posteriormente, se diseñará una estrategia para seleccionar e integrar técnicas de compresión sin pérdida de precisión existentes, seguida de su implementación y optimización en el simulador elegido. Finalmente, se realizarán experimentos controlados para evaluar el impacto de la compresión en términos de ahorro de memoria, tiempo de simulación y fidelidad del resultado, comparándolo con enfoques tradicionales. En las siguientes secciones, se detallarán cada una de estas etapas y su contribución al objetivo final del proyecto.